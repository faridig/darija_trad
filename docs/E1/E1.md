# Documentation Technique - E1 : Gestion des données

## 1. Vue d'ensemble
L'ensemble E1 concerne la gestion des données de traduction Darija ↔️ Français/Anglais. Cette documentation détaille l'extraction, le nettoyage et l'analyse des données du dataset MBZUAI-Paris/Darija-SFT-Mixture.

## 2. Structure du Projet

### 2.1 Organisation des Dossiers
```
data/darija_sft_mixture/
├── nettoyage/           # Scripts de nettoyage des données
├── parquet_download/    # Téléchargement des fichiers Parquet
└── statistics/          # Analyse des statistiques du dataset
```

### 2.2 Configuration Requise
```bash
# Variables d'environnement nécessaires
HUGGINGFACE_TOKEN=<token>
AZURE_STORAGE_ACCOUNT_NAME=<compte>
AZURE_STORAGE_ACCOUNT_KEY=<clé>
AZURE_CONTAINER_NAME=<container>

# JAR Hadoop requis
~/hadoop_jars/
├── hadoop-azure-3.3.1.jar
├── azure-storage-8.6.6.jar
├── jetty-util-9.4.40.v20210413.jar
└── jetty-util-ajax-9.4.40.v20210413.jar
```

## 3. Téléchargement des Données

### 3.1 Téléchargement depuis HuggingFace
```python
class DarijaParquetUploader:
    """
    Télécharge les fichiers Parquet du dataset Darija depuis Hugging Face
    et les envoie sur Azure Blob Storage.
    """
    def stream_to_azure(self, file_name):
        file_stream = BytesIO()
        local_path = hf_hub_download(
            repo_id=self.dataset_id,
            filename=file_name,
            token=self.hf_token
        )
```

### 3.2 Stockage sur Azure
```python
def load_raw_data():
    """
    Lit les données Parquet depuis Azure et les filtre par direction
    """
    directions = ["en_dr", "fr_dr", "dr_fr", "dr_en"]
    azure_url = f"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{parquet_folder}"
    
    df = spark.read.parquet(azure_url).filter(col("direction").isin(directions))
    df_local = df.coalesce(4).toPandas()
    
    output_dir = "data_Darija-SFT-Mixture/darija_data"
    os.makedirs(output_dir, exist_ok=True)
    raw_file = os.path.join(output_dir, "donnees_brutes.csv")
    
    df_local.to_csv(raw_file, index=False)
    return raw_file
```

## 4. Nettoyage des Données

### 4.1 Règles de Nettoyage
```python
def clean_text(text):
    """
    Nettoyage unifié du texte :
    1. Suppression préfixes arabes (ترجم …:)
    2. Normalisation des espaces
    3. Correction des caractères spéciaux
    """
    # Supprime "ترجم ...:"
    text = re.sub(prefix_pattern, "", text)
    # Normalisation des espaces
    text = text.replace("\\xa0", " ")
    text = text.replace("\xa0", " ")
    text = text.replace("\u2009", " ")
    # Correction des caractères
    text = text.replace("\\'", "'")
    text = text.replace("\\", " ")
    
    return text.strip()
```

### 4.2 Extraction des Paires
```python
def extract_pairs(row_str):
    """
    Transforme une chaîne Row(content='...', role='...') 
    en liste de paires { 'texte_cible': str, 'traduction': str }
    """
    pattern = r"Row\(content=(?P<quote>['\"])(?P<content>.*?)(?P=quote),\s*role=['\"](?P<role>.*?)['\"]\)"
    matches = re.findall(pattern, row_str, re.DOTALL)
    
    pairs = []
    for _, content, role in matches:
        clean_content = clean_text(content.replace("\\n", ""))
        pairs.append({"role": role, "content": clean_content})
    
    return pairs
```

## 5. Statistiques du Dataset

### 5.1 Collecte des Statistiques
```python
class DarijaStatsAPI:
    """
    Récupère et analyse les statistiques du dataset via l'API Hugging Face
    """
    def get_dataset_info(self):
        response = requests.get(self.api_url, headers=self.headers)
        info = response.json()
        
        files_url = f"{self.api_url}/parquet"
        files_response = requests.get(files_url, headers=self.headers)
        info['files'] = files_response.json()
        
        return info
```

### 5.2 Format des Statistiques
```python
def prepare_stats(self, info):
    """Format des statistiques générées"""
    return {
        "dataset": {
            "nom": info.get('id'),
            "description": info.get('description'),
            "licence": info.get('cardData', {}).get('license'),
            "mis_à_jour": info.get('lastModified')
        },
        "contenu": {
            "format": "Parquet",
            "fichiers": len(info.get('files', []))
        }
    }
```

## 6. Utilisation

### 6.1 Traitement des Données
```python
from data.darija_sft_mixture.nettoyage.nettoyage_csv import process_csv

process_csv(
    csv_file="data_Darija-SFT-Mixture/darija_data/donnees_brutes.csv",
    output_json="data_Darija-SFT-Mixture/darija_data/traductions_processed.json",
    error_csv="data_Darija-SFT-Mixture/darija_data/lignes_problematiques.csv"
)
```

### 6.2 Analyse des Statistiques
```python
from data.darija_sft_mixture.statistics.dataset_statistics import DarijaStatsAPI
from pathlib import Path

api = DarijaStatsAPI(Path("."))
api.run()  # Génère dataset_stats.json
``` 