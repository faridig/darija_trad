{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projets/darija_app_finetune/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Base     : غاديا غاديا يطيح.\n",
      "🔸 LoRA     : غادي الشتا غدّا.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "text_fr = \"Il va pleuvoir demain.\"\n",
    "\n",
    "# Tokenizer partagé\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "tokenizer.src_lang = \"fra_Latn\"\n",
    "inputs = tokenizer(text_fr, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Token de début darija\n",
    "forced_bos_token_id = tokenizer.convert_tokens_to_ids(\"ary_Arab\")\n",
    "\n",
    "# A – modèle original\n",
    "model_base = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "outputs_base = model_base.generate(\n",
    "    **inputs, forced_bos_token_id=forced_bos_token_id, max_length=128\n",
    ")\n",
    "translation_base = tokenizer.batch_decode(outputs_base, skip_special_tokens=True)[0]\n",
    "\n",
    "# B – modèle fine-tuné LoRA\n",
    "model_lora = AutoModelForSeq2SeqLM.from_pretrained(\"nllb-darija-lora-model\")\n",
    "outputs_lora = model_lora.generate(\n",
    "    **inputs, forced_bos_token_id=forced_bos_token_id, max_length=128\n",
    ")\n",
    "translation_lora = tokenizer.batch_decode(outputs_lora, skip_special_tokens=True)[0]\n",
    "\n",
    "# Résultats\n",
    "print(\"🔹 Base     :\", translation_base)\n",
    "print(\"🔸 LoRA     :\", translation_lora)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 BLEU modèle original : 1.37\n",
      "📊 BLEU modèle LoRA     : 5.94\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Charger les deux modèles\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model_base = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model_lora = AutoModelForSeq2SeqLM.from_pretrained(\"nllb-darija-lora-model\")\n",
    "\n",
    "# BLEU\n",
    "bleu = load(\"sacrebleu\")\n",
    "\n",
    "# Dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"all_translations_dataset.json\", split=\"train[:100]\")\n",
    "\n",
    "preds_base, preds_lora, refs = [], [], []\n",
    "\n",
    "tokenizer.src_lang = \"fra_Latn\"\n",
    "forced_bos_token_id = tokenizer.convert_tokens_to_ids(\"ary_Arab\")\n",
    "\n",
    "for example in dataset:\n",
    "    translation = example[\"translation\"]\n",
    "\n",
    "    src = translation.get(\"fra_Latn\", None)\n",
    "    tgt = translation.get(\"ary_Arab\", None)\n",
    "\n",
    "    if src is None or tgt is None:\n",
    "        continue  # 🛑 Skip si une langue est absente\n",
    "\n",
    "    # Tokeniser entrée\n",
    "    inputs = tokenizer(src, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Génération modèle base\n",
    "    out_base = model_base.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "    pred_base = tokenizer.decode(out_base[0], skip_special_tokens=True)\n",
    "\n",
    "    # Génération modèle LoRA\n",
    "    out_lora = model_lora.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "    pred_lora = tokenizer.decode(out_lora[0], skip_special_tokens=True)\n",
    "\n",
    "    preds_base.append(pred_base)\n",
    "    preds_lora.append(pred_lora)\n",
    "    refs.append([tgt])\n",
    "\n",
    "# BLEU Score\n",
    "score_base = bleu.compute(predictions=preds_base, references=refs)\n",
    "score_lora = bleu.compute(predictions=preds_lora, references=refs)\n",
    "\n",
    "print(f\"📊 BLEU modèle original : {score_base['score']:.2f}\")\n",
    "print(f\"📊 BLEU modèle LoRA     : {score_lora['score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ EXEMPLE 1 ================\n",
      "🧾 FRANÇAIS      : Tout ce qu'il fait, c'est nous rabaisser et nous rappeler qu'il est intellectuellement supérieur\n",
      "🎯 RÉFÉRENCE     : لحاجا لواحيدا لي كايدير هيا كايطيّح منّا, وي فكّرنا أنّاهو موتقّاف علينا\n",
      "🔹 ORIGINAL      : كَيْدِيرْ كُلّْشِي بَاشْ يْخْلِّينَا نْتّْهَيّْنُو وْيْتّْفَكّْرُو بْلِّي هُوَ أَعْظِمْ فْالْفْهْمَة.\n",
      "🔸 LoRA FINE-TUNE: لحاجا ليا كايدير هووا هووا هووا كايخايبنا أُ كايعزز علينا بلي هووّا فاقلي\n",
      "\n",
      "================ EXEMPLE 2 ================\n",
      "🧾 FRANÇAIS      : En somme, la Renaissance a été à l'origine d'un changement majeur dans la façon d'envisager l'apprentissage et la diffusion des connaissances.\n",
      "🎯 RÉFÉRENCE     : وفي جوهر الأمر، دار عصر النهضة تغيير كبير في نهج التعلم ونشر المعرفة.\n",
      "🔹 ORIGINAL      : وْبْالْكْتَابْ، رَاهْ التّْوْقِيَّة دْيَالْ النّْقْدَامَة دْيَالْهَا دَارَاتْ تّْبْدِيلْ فْالْمْقَادَة دْيَالْ التَّعْلِيمْ وْالْمَعْرِفَة.\n",
      "🔸 LoRA FINE-TUNE: في الختصار، أدى عصر النهضة إلى تغيير كبير في طريقة التعلم ونشر المعرفة.\n",
      "\n",
      "================ EXEMPLE 4 ================\n",
      "🧾 FRANÇAIS      : Sais-tu te diriger en fonction de la position du soleil ?\n",
      "🎯 RÉFÉRENCE     : واش كاتعراف كيفاش توجّه راسّاك غير بماكان شمش?\n",
      "🔹 ORIGINAL      : واش كايقدر تديرك على حسب الموقع ديال الشمس؟\n",
      "🔸 LoRA FINE-TUNE: واش كاتعرف كيفاش تڭدّي راسك علا طريق لموڭازا ديال شمش?\n",
      "\n",
      "================ EXEMPLE 9 ================\n",
      "🧾 FRANÇAIS      : ouvre le frigo\n",
      "🎯 RÉFÉRENCE     : حلّ التلّاجا\n",
      "🔹 ORIGINAL      : ففتحي فالجليد\n",
      "🔸 LoRA FINE-TUNE: حلّ لفرّاجا\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import random\n",
    "\n",
    "# Charger les modèles\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model_base = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model_lora = AutoModelForSeq2SeqLM.from_pretrained(\"nllb-darija-lora-model\")\n",
    "\n",
    "# Charger le dataset complet\n",
    "dataset = load_dataset(\"json\", data_files=\"all_translations_dataset.json\", split=\"train\")\n",
    "tokenizer.src_lang = \"fra_Latn\"\n",
    "forced_bos_token_id = tokenizer.convert_tokens_to_ids(\"ary_Arab\")\n",
    "\n",
    "# Mélanger le dataset et en prendre 10\n",
    "examples = dataset.shuffle(seed=42).select(range(10))\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    translation = example[\"translation\"]\n",
    "    src = translation.get(\"fra_Latn\", None)\n",
    "    tgt = translation.get(\"ary_Arab\", None)\n",
    "\n",
    "    if src is None or tgt is None:\n",
    "        continue\n",
    "\n",
    "    inputs = tokenizer(src, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Génération modèle original\n",
    "    out_base = model_base.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "    pred_base = tokenizer.decode(out_base[0], skip_special_tokens=True)\n",
    "\n",
    "    # Génération modèle LoRA\n",
    "    out_lora = model_lora.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "    pred_lora = tokenizer.decode(out_lora[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n================ EXEMPLE {idx+1} ================\")\n",
    "    print(f\"🧾 FRANÇAIS      : {src}\")\n",
    "    print(f\"🎯 RÉFÉRENCE     : {tgt}\")\n",
    "    print(f\"🔹 ORIGINAL      : {pred_base}\")\n",
    "    print(f\"🔸 LoRA FINE-TUNE: {pred_lora}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
