{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projets/darija_app_finetune/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ Base     : ØºØ§Ø¯ÙŠØ§ ØºØ§Ø¯ÙŠØ§ ÙŠØ·ÙŠØ­.\n",
      "ğŸ”¸ LoRA     : ØºØ§Ø¯ÙŠ Ø§Ù„Ø´ØªØ§ ØºØ¯Ù‘Ø§.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "text_fr = \"Il va pleuvoir demain.\"\n",
    "\n",
    "# Tokenizer partagÃ©\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "tokenizer.src_lang = \"fra_Latn\"\n",
    "inputs = tokenizer(text_fr, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Token de dÃ©but darija\n",
    "forced_bos_token_id = tokenizer.convert_tokens_to_ids(\"ary_Arab\")\n",
    "\n",
    "# A â€“ modÃ¨le original\n",
    "model_base = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "outputs_base = model_base.generate(\n",
    "    **inputs, forced_bos_token_id=forced_bos_token_id, max_length=128\n",
    ")\n",
    "translation_base = tokenizer.batch_decode(outputs_base, skip_special_tokens=True)[0]\n",
    "\n",
    "# B â€“ modÃ¨le fine-tunÃ© LoRA\n",
    "model_lora = AutoModelForSeq2SeqLM.from_pretrained(\"nllb-darija-lora-model\")\n",
    "outputs_lora = model_lora.generate(\n",
    "    **inputs, forced_bos_token_id=forced_bos_token_id, max_length=128\n",
    ")\n",
    "translation_lora = tokenizer.batch_decode(outputs_lora, skip_special_tokens=True)[0]\n",
    "\n",
    "# RÃ©sultats\n",
    "print(\"ğŸ”¹ Base     :\", translation_base)\n",
    "print(\"ğŸ”¸ LoRA     :\", translation_lora)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š BLEU modÃ¨le original : 1.37\n",
      "ğŸ“Š BLEU modÃ¨le LoRA     : 5.94\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Charger les deux modÃ¨les\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model_base = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model_lora = AutoModelForSeq2SeqLM.from_pretrained(\"nllb-darija-lora-model\")\n",
    "\n",
    "# BLEU\n",
    "bleu = load(\"sacrebleu\")\n",
    "\n",
    "# Dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"all_translations_dataset.json\", split=\"train[:100]\")\n",
    "\n",
    "preds_base, preds_lora, refs = [], [], []\n",
    "\n",
    "tokenizer.src_lang = \"fra_Latn\"\n",
    "forced_bos_token_id = tokenizer.convert_tokens_to_ids(\"ary_Arab\")\n",
    "\n",
    "for example in dataset:\n",
    "    translation = example[\"translation\"]\n",
    "\n",
    "    src = translation.get(\"fra_Latn\", None)\n",
    "    tgt = translation.get(\"ary_Arab\", None)\n",
    "\n",
    "    if src is None or tgt is None:\n",
    "        continue  # ğŸ›‘ Skip si une langue est absente\n",
    "\n",
    "    # Tokeniser entrÃ©e\n",
    "    inputs = tokenizer(src, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # GÃ©nÃ©ration modÃ¨le base\n",
    "    out_base = model_base.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "    pred_base = tokenizer.decode(out_base[0], skip_special_tokens=True)\n",
    "\n",
    "    # GÃ©nÃ©ration modÃ¨le LoRA\n",
    "    out_lora = model_lora.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "    pred_lora = tokenizer.decode(out_lora[0], skip_special_tokens=True)\n",
    "\n",
    "    preds_base.append(pred_base)\n",
    "    preds_lora.append(pred_lora)\n",
    "    refs.append([tgt])\n",
    "\n",
    "# BLEU Score\n",
    "score_base = bleu.compute(predictions=preds_base, references=refs)\n",
    "score_lora = bleu.compute(predictions=preds_lora, references=refs)\n",
    "\n",
    "print(f\"ğŸ“Š BLEU modÃ¨le original : {score_base['score']:.2f}\")\n",
    "print(f\"ğŸ“Š BLEU modÃ¨le LoRA     : {score_lora['score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ EXEMPLE 1 ================\n",
      "ğŸ§¾ FRANÃ‡AIS      : Tout ce qu'il fait, c'est nous rabaisser et nous rappeler qu'il est intellectuellement supÃ©rieur\n",
      "ğŸ¯ RÃ‰FÃ‰RENCE     : Ù„Ø­Ø§Ø¬Ø§ Ù„ÙˆØ§Ø­ÙŠØ¯Ø§ Ù„ÙŠ ÙƒØ§ÙŠØ¯ÙŠØ± Ù‡ÙŠØ§ ÙƒØ§ÙŠØ·ÙŠÙ‘Ø­ Ù…Ù†Ù‘Ø§, ÙˆÙŠ ÙÙƒÙ‘Ø±Ù†Ø§ Ø£Ù†Ù‘Ø§Ù‡Ùˆ Ù…ÙˆØªÙ‚Ù‘Ø§Ù Ø¹Ù„ÙŠÙ†Ø§\n",
      "ğŸ”¹ ORIGINAL      : ÙƒÙÙŠÙ’Ø¯ÙÙŠØ±Ù’ ÙƒÙÙ„Ù‘Ù’Ø´ÙÙŠ Ø¨ÙØ§Ø´Ù’ ÙŠÙ’Ø®Ù’Ù„Ù‘ÙÙŠÙ†ÙØ§ Ù†Ù’ØªÙ‘Ù’Ù‡ÙÙŠÙ‘Ù’Ù†ÙÙˆ ÙˆÙ’ÙŠÙ’ØªÙ‘Ù’ÙÙÙƒÙ‘Ù’Ø±ÙÙˆ Ø¨Ù’Ù„Ù‘ÙÙŠ Ù‡ÙÙˆÙ Ø£ÙØ¹Ù’Ø¸ÙÙ…Ù’ ÙÙ’Ø§Ù„Ù’ÙÙ’Ù‡Ù’Ù…ÙØ©.\n",
      "ğŸ”¸ LoRA FINE-TUNE: Ù„Ø­Ø§Ø¬Ø§ Ù„ÙŠØ§ ÙƒØ§ÙŠØ¯ÙŠØ± Ù‡ÙˆÙˆØ§ Ù‡ÙˆÙˆØ§ Ù‡ÙˆÙˆØ§ ÙƒØ§ÙŠØ®Ø§ÙŠØ¨Ù†Ø§ Ø£Ù ÙƒØ§ÙŠØ¹Ø²Ø² Ø¹Ù„ÙŠÙ†Ø§ Ø¨Ù„ÙŠ Ù‡ÙˆÙˆÙ‘Ø§ ÙØ§Ù‚Ù„ÙŠ\n",
      "\n",
      "================ EXEMPLE 2 ================\n",
      "ğŸ§¾ FRANÃ‡AIS      : En somme, la Renaissance a Ã©tÃ© Ã  l'origine d'un changement majeur dans la faÃ§on d'envisager l'apprentissage et la diffusion des connaissances.\n",
      "ğŸ¯ RÃ‰FÃ‰RENCE     : ÙˆÙÙŠ Ø¬ÙˆÙ‡Ø± Ø§Ù„Ø£Ù…Ø±ØŒ Ø¯Ø§Ø± Ø¹ØµØ± Ø§Ù„Ù†Ù‡Ø¶Ø© ØªØºÙŠÙŠØ± ÙƒØ¨ÙŠØ± ÙÙŠ Ù†Ù‡Ø¬ Ø§Ù„ØªØ¹Ù„Ù… ÙˆÙ†Ø´Ø± Ø§Ù„Ù…Ø¹Ø±ÙØ©.\n",
      "ğŸ”¹ ORIGINAL      : ÙˆÙ’Ø¨Ù’Ø§Ù„Ù’ÙƒÙ’ØªÙØ§Ø¨Ù’ØŒ Ø±ÙØ§Ù‡Ù’ Ø§Ù„ØªÙ‘Ù’ÙˆÙ’Ù‚ÙÙŠÙ‘ÙØ© Ø¯Ù’ÙŠÙØ§Ù„Ù’ Ø§Ù„Ù†Ù‘Ù’Ù‚Ù’Ø¯ÙØ§Ù…ÙØ© Ø¯Ù’ÙŠÙØ§Ù„Ù’Ù‡ÙØ§ Ø¯ÙØ§Ø±ÙØ§ØªÙ’ ØªÙ‘Ù’Ø¨Ù’Ø¯ÙÙŠÙ„Ù’ ÙÙ’Ø§Ù„Ù’Ù…Ù’Ù‚ÙØ§Ø¯ÙØ© Ø¯Ù’ÙŠÙØ§Ù„Ù’ Ø§Ù„ØªÙ‘ÙØ¹Ù’Ù„ÙÙŠÙ…Ù’ ÙˆÙ’Ø§Ù„Ù’Ù…ÙØ¹Ù’Ø±ÙÙÙØ©.\n",
      "ğŸ”¸ LoRA FINE-TUNE: ÙÙŠ Ø§Ù„Ø®ØªØµØ§Ø±ØŒ Ø£Ø¯Ù‰ Ø¹ØµØ± Ø§Ù„Ù†Ù‡Ø¶Ø© Ø¥Ù„Ù‰ ØªØºÙŠÙŠØ± ÙƒØ¨ÙŠØ± ÙÙŠ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ¹Ù„Ù… ÙˆÙ†Ø´Ø± Ø§Ù„Ù…Ø¹Ø±ÙØ©.\n",
      "\n",
      "================ EXEMPLE 4 ================\n",
      "ğŸ§¾ FRANÃ‡AIS      : Sais-tu te diriger en fonction de la position du soleil ?\n",
      "ğŸ¯ RÃ‰FÃ‰RENCE     : ÙˆØ§Ø´ ÙƒØ§ØªØ¹Ø±Ø§Ù ÙƒÙŠÙØ§Ø´ ØªÙˆØ¬Ù‘Ù‡ Ø±Ø§Ø³Ù‘Ø§Ùƒ ØºÙŠØ± Ø¨Ù…Ø§ÙƒØ§Ù† Ø´Ù…Ø´?\n",
      "ğŸ”¹ ORIGINAL      : ÙˆØ§Ø´ ÙƒØ§ÙŠÙ‚Ø¯Ø± ØªØ¯ÙŠØ±Ùƒ Ø¹Ù„Ù‰ Ø­Ø³Ø¨ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø¯ÙŠØ§Ù„ Ø§Ù„Ø´Ù…Ø³ØŸ\n",
      "ğŸ”¸ LoRA FINE-TUNE: ÙˆØ§Ø´ ÙƒØ§ØªØ¹Ø±Ù ÙƒÙŠÙØ§Ø´ ØªÚ­Ø¯Ù‘ÙŠ Ø±Ø§Ø³Ùƒ Ø¹Ù„Ø§ Ø·Ø±ÙŠÙ‚ Ù„Ù…ÙˆÚ­Ø§Ø²Ø§ Ø¯ÙŠØ§Ù„ Ø´Ù…Ø´?\n",
      "\n",
      "================ EXEMPLE 9 ================\n",
      "ğŸ§¾ FRANÃ‡AIS      : ouvre le frigo\n",
      "ğŸ¯ RÃ‰FÃ‰RENCE     : Ø­Ù„Ù‘ Ø§Ù„ØªÙ„Ù‘Ø§Ø¬Ø§\n",
      "ğŸ”¹ ORIGINAL      : ÙÙØªØ­ÙŠ ÙØ§Ù„Ø¬Ù„ÙŠØ¯\n",
      "ğŸ”¸ LoRA FINE-TUNE: Ø­Ù„Ù‘ Ù„ÙØ±Ù‘Ø§Ø¬Ø§\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import random\n",
    "\n",
    "# Charger les modÃ¨les\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model_base = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model_lora = AutoModelForSeq2SeqLM.from_pretrained(\"nllb-darija-lora-model\")\n",
    "\n",
    "# Charger le dataset complet\n",
    "dataset = load_dataset(\"json\", data_files=\"all_translations_dataset.json\", split=\"train\")\n",
    "tokenizer.src_lang = \"fra_Latn\"\n",
    "forced_bos_token_id = tokenizer.convert_tokens_to_ids(\"ary_Arab\")\n",
    "\n",
    "# MÃ©langer le dataset et en prendre 10\n",
    "examples = dataset.shuffle(seed=42).select(range(10))\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    translation = example[\"translation\"]\n",
    "    src = translation.get(\"fra_Latn\", None)\n",
    "    tgt = translation.get(\"ary_Arab\", None)\n",
    "\n",
    "    if src is None or tgt is None:\n",
    "        continue\n",
    "\n",
    "    inputs = tokenizer(src, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # GÃ©nÃ©ration modÃ¨le original\n",
    "    out_base = model_base.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "    pred_base = tokenizer.decode(out_base[0], skip_special_tokens=True)\n",
    "\n",
    "    # GÃ©nÃ©ration modÃ¨le LoRA\n",
    "    out_lora = model_lora.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "    pred_lora = tokenizer.decode(out_lora[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n================ EXEMPLE {idx+1} ================\")\n",
    "    print(f\"ğŸ§¾ FRANÃ‡AIS      : {src}\")\n",
    "    print(f\"ğŸ¯ RÃ‰FÃ‰RENCE     : {tgt}\")\n",
    "    print(f\"ğŸ”¹ ORIGINAL      : {pred_base}\")\n",
    "    print(f\"ğŸ”¸ LoRA FINE-TUNE: {pred_lora}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
